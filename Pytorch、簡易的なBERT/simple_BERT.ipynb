{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**ライブラリのインストール**\n",
        "\n",
        "PyTorch-Transformers、および必要なライブラリのインストールを行います。"
      ],
      "metadata": {
        "id": "GAV8SHi3z2eI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQ6FiNdzruBX",
        "outputId": "4ed1f31f-9d36-41dd-cbc6-f064fe24fe88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting folium==0.2.1\n",
            "  Downloading folium-0.2.1.tar.gz (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.0/70.0 kB\u001b[0m \u001b[31m957.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Jinja2 in /usr/local/lib/python3.10/dist-packages (from folium==0.2.1) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2->folium==0.2.1) (2.1.5)\n",
            "Building wheels for collected packages: folium\n",
            "  Building wheel for folium (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for folium: filename=folium-0.2.1-py3-none-any.whl size=79794 sha256=e4c10aee07fbe160e0c99fa20f5803588e778ee35616393f214e7a6d625edba7\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/0c/07/d7792a5444d5bb074361ac27da53cee9d5cce59a07fe9da5dd\n",
            "Successfully built folium\n",
            "Installing collected packages: folium\n",
            "  Attempting uninstall: folium\n",
            "    Found existing installation: folium 0.14.0\n",
            "    Uninstalling folium-0.14.0:\n",
            "      Successfully uninstalled folium-0.14.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.17.6 requires folium>=0.9.1, but you have folium 0.2.1 which is incompatible.\n",
            "geemap 0.32.0 requires folium>=0.13.0, but you have folium 0.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed folium-0.2.1\n",
            "Collecting urllib3==1.25.11\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.0/128.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: urllib3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "Successfully installed urllib3-1.25.11\n",
            "Collecting pytorch-transformers==1.2.0\n",
            "  Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.4/176.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers==1.2.0) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers==1.2.0) (1.25.2)\n",
            "Collecting boto3 (from pytorch-transformers==1.2.0)\n",
            "  Downloading boto3-1.34.65-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers==1.2.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers==1.2.0) (4.66.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers==1.2.0) (2023.12.25)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers==1.2.0) (0.1.99)\n",
            "Collecting sacremoses (from pytorch-transformers==1.2.0)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers==1.2.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers==1.2.0) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers==1.2.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers==1.2.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers==1.2.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers==1.2.0) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.0.0->pytorch-transformers==1.2.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.0.0->pytorch-transformers==1.2.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.0.0->pytorch-transformers==1.2.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.0.0->pytorch-transformers==1.2.0)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.0.0->pytorch-transformers==1.2.0)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.0.0->pytorch-transformers==1.2.0)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.0.0->pytorch-transformers==1.2.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.0.0->pytorch-transformers==1.2.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.0.0->pytorch-transformers==1.2.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.0.0->pytorch-transformers==1.2.0)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.0.0->pytorch-transformers==1.2.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers==1.2.0) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->pytorch-transformers==1.2.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.35.0,>=1.34.65 (from boto3->pytorch-transformers==1.2.0)\n",
            "  Downloading botocore-1.34.65-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->pytorch-transformers==1.2.0)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->pytorch-transformers==1.2.0)\n",
            "  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-transformers==1.2.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-transformers==1.2.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-transformers==1.2.0) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-transformers==1.2.0) (2024.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->pytorch-transformers==1.2.0) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->pytorch-transformers==1.2.0) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.65->boto3->pytorch-transformers==1.2.0) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->pytorch-transformers==1.2.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->pytorch-transformers==1.2.0) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.65->boto3->pytorch-transformers==1.2.0) (1.16.0)\n",
            "Installing collected packages: sacremoses, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jmespath, nvidia-cusparse-cu12, nvidia-cudnn-cu12, botocore, s3transfer, nvidia-cusolver-cu12, boto3, pytorch-transformers\n",
            "Successfully installed boto3-1.34.65 botocore-1.34.65 jmespath-1.0.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 pytorch-transformers-1.2.0 s3transfer-0.10.1 sacremoses-0.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install folium==0.2.1\n",
        "!pip install urllib3==1.25.11\n",
        "!pip install pytorch-transformers==1.2.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**文章の一部の予測**\n",
        "\n",
        "文章における一部の単語をMASKし、それをBERTのモデルを使って予測します。"
      ],
      "metadata": {
        "id": "_3DGPs7yz8xZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "from pytorch_transformers import BertForMaskedLM\n",
        "from pytorch_transformers import BertTokenizer\n",
        "\n",
        "\n",
        "text = \"[CLS] I played baseball with my friends at school yesterday [SEP]\"\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "words = tokenizer.tokenize(text)\n",
        "print(words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDgeQwAjrvAs",
        "outputId": "c963af32-0cd2-4272-a7f5-16ce071bcdca"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 1308952.41B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'i', 'played', 'baseball', 'with', 'my', 'friends', 'at', 'school', 'yesterday', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "文章の一部をMASKします。"
      ],
      "metadata": {
        "id": "hxIOTwcD0Mkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "msk_idx = 3\n",
        "words[msk_idx] = \"[MASK]\"  # 単語を[MASK]に置き換える\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJq6aTFWrxh9",
        "outputId": "7d6e12e0-5c3c-4791-9803-ccc0eac335a9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'i', 'played', '[MASK]', 'with', 'my', 'friends', 'at', 'school', 'yesterday', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "単語を対応するインデックスに変換します。"
      ],
      "metadata": {
        "id": "voWsGBRh0YJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_ids = tokenizer.convert_tokens_to_ids(words)  # 単語をインデックスに変換\n",
        "word_tensor = torch.tensor([word_ids])  # テンソルに変換\n",
        "print(word_tensor)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfMxHZIcryD5",
        "outputId": "fad8a137-6def-4f49-ce69-58ecc903cfff"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 101, 1045, 2209,  103, 2007, 2026, 2814, 2012, 2082, 7483,  102]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERTのモデルを使って予測を行います。"
      ],
      "metadata": {
        "id": "hkNkTL4M0iMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "msk_model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "msk_model.cuda()  # GPU対応\n",
        "msk_model.eval()\n",
        "\n",
        "x = word_tensor.cuda()  # GPU対応\n",
        "y = msk_model(x)  # 予測\n",
        "result = y[0]\n",
        "print(result.size())  # 結果の形状\n",
        "\n",
        "_, max_ids = torch.topk(result[0][msk_idx], k=5)  # 最も大きい5つの値\n",
        "result_words = tokenizer.convert_ids_to_tokens(max_ids.tolist())  # インデックスを単語に変換\n",
        "print(result_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_C8iIEvKr0yf",
        "outputId": "2b1527b3-bf47-4c7d-fdd9-51c2df99dba2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 433/433 [00:00<00:00, 233436.20B/s]\n",
            "100%|██████████| 440473133/440473133 [00:12<00:00, 34906688.90B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 11, 30522])\n",
            "['basketball', 'football', 'soccer', 'baseball', 'tennis']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **文章が連続しているかどうかの判定**\n",
        "\n",
        "BERTのモデルを使って、2つの文章が連続しているかどうかの判定を行います。\n",
        "以下の関数show_continuityでは、2つの文章の連続性を判定し、表示します。"
      ],
      "metadata": {
        "id": "o31qJicG0reN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_transformers import BertForNextSentencePrediction\n",
        "\n",
        "def show_continuity(text, seg_ids):\n",
        "    words = tokenizer.tokenize(text)\n",
        "    word_ids = tokenizer.convert_tokens_to_ids(words)  # 単語をインデックスに変換\n",
        "    word_tensor = torch.tensor([word_ids])  # テンソルに変換\n",
        "\n",
        "    seg_tensor = torch.tensor([seg_ids])\n",
        "\n",
        "    nsp_model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
        "    nsp_model.cuda()  # GPU対応\n",
        "    nsp_model.eval()\n",
        "\n",
        "    x = word_tensor.cuda()  # GPU対応\n",
        "    s = seg_tensor.cuda()  # GPU対応\n",
        "\n",
        "    y = nsp_model(x, s)  # 予測\n",
        "    result = torch.softmax(y[0], dim=1)\n",
        "    print(result)  # Softmaxで確率に\n",
        "    print(str(result[0][0].item()*100) + \"%の確率で連続しています。\")"
      ],
      "metadata": {
        "id": "cV3b0w94r3UJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "show_continuity関数に、自然につながる2つの文章を与えます。"
      ],
      "metadata": {
        "id": "3t5vD7iq066I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"[CLS] What is baseball ? [SEP] It is a game of hitting the ball with the bat [SEP]\"\n",
        "seg_ids = [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ,1, 1]  # 0:前の文章の単語、1:後の文章の単語\n",
        "show_continuity(text, seg_ids)"
      ],
      "metadata": {
        "id": "xt0C8D-r5YFM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00231518-0d2a-4e4a-8da7-ea866f01f070"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000e+00, 4.5869e-06]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "99.9995470046997%の確率で連続しています。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "show_continuity関数に、自然につながらない2つの文章を与えます。"
      ],
      "metadata": {
        "id": "lFkGcdnY1Bzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"[CLS] What is baseball ? [SEP] This food is made with flour and milk [SEP]\"\n",
        "seg_ids=[0,0,0,0,0,0,1,1,1,1,1,1,1,1,1]\n",
        "show_continuity(text,seg_ids)"
      ],
      "metadata": {
        "id": "EsAFEwiA5oyg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "388eb7b4-c528-43ac-d514-23aaa15dbf07"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[9.5296e-06, 9.9999e-01]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "0.0009529620001558214%の確率で連続しています。\n"
          ]
        }
      ]
    }
  ]
}